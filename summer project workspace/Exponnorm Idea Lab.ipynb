{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f0cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import scipy as sp\n",
    "import scipy.special\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8f980",
   "metadata": {},
   "source": [
    "### Modifying training on the second teacher\n",
    "Our new hypothesis is prioritising replay of rare and high reward inputs reduces forgetting. We can think of rare inputs (i.e. inputs which have strong presence in the input distribution of Task A but are uncommon in Task B) as similar to replaying unexplored regions. Mice have been shown to replay and imagine parts of a maze which they have seen but not explored e.g. they saw a left turn in T interchange but went right instead. \n",
    "\n",
    "We can modify the get_batch() function in the training_step() function.\n",
    "\n",
    "Inline below, I copy the current get_batch function in our exponnorm and modify it to accept our values. \n",
    "\n",
    "Pipeline: Network_Runner\n",
    "            -> train():\n",
    "            ->train_on_step\n",
    "            ->training_step:\n",
    "                ->get_batch (called from a data_module as specified by the config)\n",
    "                \n",
    "\n",
    "We want the data distribution of the second input to shift each time, so that we cover a different section of the distribution. \n",
    "\n",
    "How do we want to systematically vary this filter?\n",
    "Ideas:\n",
    "- Very simple - Shift mean, keep K and std constant\n",
    "- More interesting - vary K and the mean.\n",
    "\n",
    "What if we could design the distribution parameters so that we have a different percentage of overlapped inputs each time?\n",
    "\n",
    "e.g Given {p1, p2} in X2 (second distribution)  which are 2+ (or 3+) std (here std refers to the standard deviation of the exponorm!) away from the mean.\n",
    "Then we can find the integral between these points and {-inf, inf} for both X1 and X2 and find the fraction of shared input probability?\n",
    "Working backwards, we could vary the fraction and thus extract the value of K giving rise to this fraction -> we have our new distribution. \n",
    "\n",
    "Frankly, testing shows that varying K only and setting a constant std could be enough for our purposes...\n",
    "\n",
    "Approach for this would be:\n",
    "- Integrate Normal Distribution & exponnorm over range defined by std\n",
    "- define fraction (one area/another) using constants and independant variable=std.\n",
    "- solve. i.e. if we want frac = 0.2, what is our value for K?\n",
    "- check how shifting the mean works with this, but it should be enough to just vary K. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "954a165b5e2e1e721a08ff56dda67b5112531629a0d1c201d72c8216f3e90fc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
